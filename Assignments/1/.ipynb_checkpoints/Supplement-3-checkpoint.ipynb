{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Supplement 3: Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.2 Programming Task: Gradient Descent\n",
        "Consider the function $J(\\boldsymbol{w})=J(w_1, w_2) =-e^{-\\frac{1}{100}(w_1^2 + w_2^2 -w_1w_2 - 2w_1  + 4w_2 + 5)} - 2 e^{-(w_1^2  + w_2^2  - 4w_1- 9 w_2 + 25)}.$\n",
        "\n",
        "i\\. Plot $J(\\boldsymbol{w})$ as a function of $\\boldsymbol{w}$ using the contour plot using the Matplotlib module. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define function J(w)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Prepare contours plot of J(w)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ii\\. Obtain the gradient of the above function by hand.\n",
        "            \n",
        "  $Hint:   \\nabla J(\\boldsymbol{w}) = \\left(\n",
        "  \\begin{array}{c}\n",
        "    \\frac{\\partial J(\\boldsymbol{w})}{\\partial w_1} \\\\ %\n",
        "    \\frac{\\partial J(\\boldsymbol{w})}{\\partial w_2}\n",
        "  \\end{array} \\right)\n",
        "$\n",
        "\n",
        "iii\\. Implement gradient descent algorithm described in the lecture to find the minimum of this\n",
        "            function using the NumPy module. Plot the location of the new $\\boldsymbol{w}$ after each iteration on the contour plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define function gradient_J(w)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Define the gradient descent algorithm as function\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Train w \n",
        "# Store each update of w in an array to make plot later.\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Prepare contour plot and mark each weight update on the plot.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 3.3 Programming Task: Housing Price Regression Problem\n",
        "The file __house\\_prices.txt__ contains a data set of house prices: the\n",
        "first column is the house size in square feet, the second column is the number\n",
        "of bedrooms, the third column is the price in USD.\n",
        "\n",
        "\n",
        "i\\. Plot house prices vs. house sizes as a scatter plot."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Read and prepare scatter plot\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ii\\. Next, fit the linear regression to these data points. You should not use\n",
        "    any other python module besides NumPy to find the weights of the model.\n",
        "\n",
        "* Consider the linear model $h_{\\boldsymbol{w}}(\\boldsymbol{x})=w_0 x_0 + w_1 x_1={\\boldsymbol{w}}^\\top\\boldsymbol{x}$,\n",
        "    where $x_1$ is the house size in the first column of __house\\_prices.txt__, $x_0=1$ by convention, $\\boldsymbol{x}={[x_0,x_1]}^\\top$, \n",
        "    and $\\boldsymbol{w}={[w_0, w_1]}^\\top$. Define the cost function on the dataset:\n",
        " \n",
        "    $J(\\boldsymbol{w})=\\frac{1}{2}\\sum_{i=1}^{n} \\left(h_{\\boldsymbol{w}}(\\boldsymbol{x}^{(i)})-y^{(i)}\\right)^2.$\n",
        "\n",
        " \n",
        "    Above, $n$ is the number of rows in __house_prices.txt__, $y^{(i)}$\n",
        "    is the house price given in the third column of the file, $\\boldsymbol{x}^{(i)} = {[1\\\n",
        "    x_1^{(i)}]}$ and $x_1^{(i)}$ is the house size from the first column and\n",
        "    $i$th row of the file. $h_{\\boldsymbol{w}}(\\cdot)$ represents the linear regression model. Plot $J(\\boldsymbol{w})$ as a function of $\\boldsymbol{w}$ using the contour\n",
        "    plot. You may rescale the data if required. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define function J(w)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Define function gradient_J(w)\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Plot the cost function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Start with some initial value $\\boldsymbol{w}$\n",
        "and run the steps of the gradient descent algorithm (you may reuse revelant\n",
        "parts of the solution from task 3.2). Plot the location of each new $\\boldsymbol{w}$ on\n",
        "the contour plot similar to the previous task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Train w \n",
        "# Store each update of w in an array to make plot later.\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Prepare contour plot and mark each weight update on the plot.\n",
        "\n",
        "\n",
        "# OPTIONAL: Plot the resulting linear regression function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "* Experiment by making changes to the learning rate of the gradient descent\n",
        "algorithm. Observe how the path of the algorithm changes. Make sure that the\n",
        "algorithm converges to the true minimum of the function $J(\\boldsymbol{w})$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Define alphas (learning rates) to test.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# TODO: Repeat experiment for each alpha and observe the contour plot in each case. You may use subplots in Matplotlib for the plots.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "iii\\. Determine the weights of the model using the closed form solution for $\\boldsymbol{w}$:\n",
        "\\begin{align}\n",
        "    \\hat{\\mathbf{w}} = \\left(\\mathbf{X}^T\\mathbf{X}\\right)^{-1}\\mathbf{X}^Ty\n",
        "\\end{align}\n",
        "Above, $\\mathbf{y} = [y^{(1)},..., y^{(n)}]^T$ and $\\mathbf{X}$ is the data matrix whose rows are $x^{(i)}$. Is this the same point that you have found above using gradient descent?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: Compute the closed form solution\n"
      ]
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "python3"
    },
    "kernelspec": {
      "display_name": "Python 3.9.6 ('mink')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "nteract": {
      "version": "0.15.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "afb0edaf3b3f70bcb49dca4bd8538b9e965e464d20941fdefd91078e019aa396"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
